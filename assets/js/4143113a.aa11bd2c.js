"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9782],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return u}});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=c(a),u=i,h=m["".concat(l,".").concat(u)]||m[u]||p[u]||o;return a?n.createElement(h,r(r({ref:t},d),{},{components:a})):n.createElement(h,r({ref:t},d))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var c=2;c<o;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},6453:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return d},default:function(){return m}});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={id:"tutorial06",title:"Building a shape model from data"},l=void 0,c={unversionedId:"Tutorials/tutorial06",id:"version-0.92/Tutorials/tutorial06",title:"Building a shape model from data",description:"The goal in this tutorial is to learn how to build a Point Distribution Model",source:"@site/versioned_docs/version-0.92/Tutorials/tutorial06.md",sourceDirName:"Tutorials",slug:"/Tutorials/tutorial06",permalink:"/docs/Tutorials/tutorial06",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/versioned_docs/version-0.92/Tutorials/tutorial06.md",tags:[],version:"0.92",frontMatter:{id:"tutorial06",title:"Building a shape model from data"},sidebar:"docs",previous:{title:"Gaussian processes, sampling and marginalization",permalink:"/docs/Tutorials/tutorial05"},next:{title:"Shape modelling with Gaussian processes and kernels",permalink:"/docs/Tutorials/tutorial07"}},d=[{value:"Related resources",id:"related-resources",children:[],level:5},{value:"Preparation",id:"preparation",children:[],level:5},{value:"Loading and preprocessing a dataset:",id:"loading-and-preprocessing-a-dataset",children:[{value:"Rigidly aligning the data:",id:"rigidly-aligning-the-data",children:[],level:4}],level:3},{value:"Building a discrete Gaussian process from data",id:"building-a-discrete-gaussian-process-from-data",children:[],level:3},{value:"A simpler method to build a model",id:"a-simpler-method-to-build-a-model",children:[],level:3}],p={toc:d};function m(e){var t=e.components,s=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,n.Z)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"The goal in this tutorial is to learn how to build a Point Distribution Model\nfrom meshes in correspondence. "),(0,o.kt)("h5",{id:"related-resources"},"Related resources"),(0,o.kt)("p",null,"To enhance your understanding of this tutorial, we recommend the following resources from our ",(0,o.kt)("a",{parentName:"p",href:"https://shapemodelling.cs.unibas.ch/ssm-course/"},"online course"),":"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Learning a model from example data ",(0,o.kt)("a",{parentName:"li",href:"https://www.futurelearn.com/courses/statistical-shape-modelling/3/steps/250329"},"(Video)"))),(0,o.kt)("p",null,"To run the code from this tutorial, download the following Scala file:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{target:"_blank",href:a(8710).Z},"Tutorial06.scala"))),(0,o.kt)("h5",{id:"preparation"},"Preparation"),(0,o.kt)("p",null,"As in the previous tutorials, we start by importing some commonly used objects and initializing the system."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"import scalismo.ui.api.*\n\nimport scalismo.geometry.*\nimport scalismo.common.*\nimport scalismo.common.interpolation.TriangleMeshInterpolator3D\nimport scalismo.mesh.*\nimport scalismo.io.{StatisticalModelIO, MeshIO}\nimport scalismo.statisticalmodel.*\nimport scalismo.registration.*\nimport scalismo.statisticalmodel.dataset.*\nimport scalismo.numerics.PivotedCholesky.RelativeTolerance\n\nimport java.io.File\n\nimport scalismo.utils.Random.FixedSeed.randBasis\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"  scalismo.initialize()\n\n  val ui = ScalismoUI()\n")),(0,o.kt)("h3",{id:"loading-and-preprocessing-a-dataset"},"Loading and preprocessing a dataset:"),(0,o.kt)("p",null,"Let's proceed to load (and visualize) a collection of face meshes. We aim to use these meshes to model the variation in shape:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'  val dsGroup = ui.createGroup("datasets")\n\n  val meshFiles = File("datasets/nonAlignedFaces/").listFiles\n  val (meshes, meshViews) = meshFiles.map(meshFile => {\n    val mesh = MeshIO.readMesh(meshFile).get\n    val meshView = ui.show(dsGroup, mesh, "mesh")\n    (mesh, meshView) // return a tuple of the mesh and the associated view\n  }).unzip // take the tuples apart, to get a sequence of meshes and one of meshViews\n')),(0,o.kt)("p",null,"What you'll notice immediately is that the meshes are not in alignment. However, something less obvious but crucial for this tutorial is that the meshes are in correspondence. This means that for every point on one of the face meshes (be it the corner of an eye, the tip of a nose, and so on), we can pinpoint the matching point on the other meshes. These corresponding points are identified by the same point ID."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Exercise: verify that the meshes are indeed in correspondence by displaying a few corresponding points.")),(0,o.kt)("h4",{id:"rigidly-aligning-the-data"},"Rigidly aligning the data:"),(0,o.kt)("p",null,"To analyze shape variations, we must remove variations caused by relative spatial displacements of the shapes, such as rotation and translation. We can accomplish this by choosing one of the meshes as a reference and aligning the rest of the datasets to it. In this instance, we'll simply use the first mesh in the list as our reference and align all the others to it."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"  val reference = meshes.head\n  val toAlign : IndexedSeq[TriangleMesh[_3D]] = meshes.tail\n")),(0,o.kt)("p",null,"Since our dataset is in correspondence, we are able to specify a set of point identifiers. These allow us to locate corresponding points across the different meshes."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'  val pointIds = IndexedSeq(2214, 6341, 10008, 14129, 8156, 47775).map(id => PointId(id))\n  val refLandmarks = pointIds.map(id => \n    Landmark(s"L_$id", reference.pointSet.point(id))\n  ) \n')),(0,o.kt)("p",null,"After locating the landmark positions on the reference, we iterate on each remaining data item, identify the corresponding landmark points and then rigidly align the mesh to the reference."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'  val alignedMeshes = toAlign.map(mesh =>\n    val landmarks = pointIds.map{id => Landmark("L_"+id, mesh.pointSet.point(id))}\n    val rigidTrans = LandmarkRegistration.rigid3DLandmarkRegistration(landmarks, refLandmarks, center = Point3D(0,0,0))\n    mesh.transform(rigidTrans)\n  )\n')),(0,o.kt)("p",null,"Now, the sequence ",(0,o.kt)("inlineCode",{parentName:"p"},"alignedMeshes"),"  holds the face meshes that have been aligned with the reference mesh."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Exercise: verify visually that at least the first element of the aligned dataset is indeed aligned to the reference.")),(0,o.kt)("h3",{id:"building-a-discrete-gaussian-process-from-data"},"Building a discrete Gaussian process from data"),(0,o.kt)("p",null,"Once we have a set of meshes that correspond and are aligned to our reference,\nwe can convert the dataset into a set of deformation fields.\nFrom these fields, we can then construct our model:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"  val defFields = alignedMeshes.map( m =>\n    val deformationVectors = reference.pointSet.pointIds.map( (id : PointId) =>\n        m.pointSet.point(id) - reference.pointSet.point(id)\n    ).toIndexedSeq\n    DiscreteField3D(reference, deformationVectors)\n  )\n")),(0,o.kt)("p",null,"From these deformation fields, we can learn the shape variations by invoking the ",(0,o.kt)("inlineCode",{parentName:"p"},"createUsingPCA")," method of the ",(0,o.kt)("inlineCode",{parentName:"p"},"DiscreteLowRankGaussianProcess")," class. Note that the deformation fields need to be interpolated, so we can confirm they are defined on all points of the reference mesh. Once we have the deformation fields, we can build and visualize the Point Distribution Model:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'  val continuousFields = defFields.map(f => f.interpolate(TriangleMeshInterpolator3D()) )\n  val gp = DiscreteLowRankGaussianProcess.createUsingPCA(\n    reference,\n    continuousFields, RelativeTolerance(1e-8)\n  )\n  val model = PointDistributionModel(gp)\n  val modelGroup = ui.createGroup("model")\n  val ssmView = ui.show(modelGroup, model, "model")\n')),(0,o.kt)("p",null,"Notice that when we visualize this mesh model in Scalismo-ui,\nit generates a GaussianProcessTransformation and the reference mesh in the\nScene Tree of Scalismo-ui."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Exercise: display the mean deformation field of the returned Gaussian Process.")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Exercise: sample and display a few deformation fields from this GP.")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Exercise: using the GP's "),"cov",(0,o.kt)("em",{parentName:"p"}," method, evaluate the sample covariance between two close points on the right cheek first, and a point on the nose and one on the cheek second. What does the data tell you?")),(0,o.kt)("h3",{id:"a-simpler-method-to-build-a-model"},"A simpler method to build a model"),(0,o.kt)("p",null,"Executing all the operations above every time we want to construct a PCA model from a set of\nfiles containing corresponding meshes can be repetitive.\nHence, Scalismo offers a simpler implementation via the ",(0,o.kt)("inlineCode",{parentName:"p"},"DataCollection")," data structure."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"DataCollection")," class in Scalismo lets you group together a dataset of corresponding meshes to facilitate collective operations on these sets."),(0,o.kt)("p",null,"We can create a ",(0,o.kt)("inlineCode",{parentName:"p"},"DataCollection")," by providing a reference mesh and a sequence of meshes that correspond to this reference."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"  val dc = DataCollection.fromTriangleMesh3DSequence(reference, alignedMeshes)\n")),(0,o.kt)("p",null,"Now that we have our data collection, we can construct a shape model as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'  val modelFromDataCollection = PointDistributionModel.createUsingPCA(dc)\n\n  val modelGroup2 = ui.createGroup("modelGroup2")\n  ui.show(modelGroup2, modelFromDataCollection, "ModelDC")\n')),(0,o.kt)("p",null,"There is a technique known as ",(0,o.kt)("em",{parentName:"p"},"Generalized Procrustes Analysis (GPA)")," that can further\nimprove the alignment of the data. It functions by computing the mean of a set of\ncorresponding surfaces, rigidly aligning all the surfaces to this mean, and iterating\nthis process until the changes in the computed mean are below a specific threshold.\nIn Scalismo, this alignment procedure is defined on data collections, and we can use it as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'  val dcWithGPAAlignedShapes = DataCollection.gpa(dc)\n  val modelFromDataCollectionGPA = PointDistributionModel.createUsingPCA(dcWithGPAAlignedShapes)\n\n  val modelGroup3 = ui.createGroup("modelGroup3")\n  ui.show(modelGroup3, modelFromDataCollectionGPA, "ModelDCGPA")\n')))}m.isMDXComponent=!0},8710:function(e,t,a){t.Z=a.p+"assets/files/Tutorial06-b97a6a4f5c4fdb240b2ba1bb5c8e9dd8.scala"}}]);